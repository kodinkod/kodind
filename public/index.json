[{"content":"Situational awareness Situational awareness.\n\u0026hellip; Before long, the world will wake up. But right now, there are perhaps a few hundred people, most of them in San Francisco and the AI labs, that have situational awareness. Through whatever peculiar forces of fate, I have found myself amongst them. A few years ago, these people were derided as crazy‚Äîbut they trusted the trendlines, which allowed them to correctly predict the AI advances of the past few years. Whether these people are also right about the next few years remains to be seen. But these are very smart people‚Äîthe smartest people I have ever met‚Äîand they are the ones building this technology. Perhaps they will be an odd footnote in history, or perhaps they will go down in history like Szilard and Oppenheimer and Teller. If they are seeing the future even close to correctly, we are in for a wild ride \u0026hellip;\nThe Scaling Hypothesis The The Scaling Hypothesis suggests that increasing the scale of neural networks (more parameters, data, and compute) improves their performance and ability to solve complex tasks. This idea is supported by models like GPT-3, which demonstrate meta-learning (few-shot learning) and generalization without complex architectures. The hypothesis implies that intelligence can emerge from simple algorithms applied at massive scales.\nAspect Description Example/Evidence Core Idea Scaling up (parameters, data, compute) improves performance, even with simple architectures. GPT-3: 175B parameters, trained on internet text, demonstrates few-shot learning. Meta-Learning Models like GPT-3 can learn new tasks from few examples, showing generalization capabilities. GPT-3 solves arithmetic and translation tasks without specialized training. Empirical Laws Performance follows power laws: improvements depend on scale of parameters, data, and compute. OpenAI‚Äôs scaling laws: loss decreases with more parameters and data. Criticism Some argue scaling alone won‚Äôt lead to true AI, as models lack long-term memory or understanding. Fran√ßois Chollet (DeepMind) highlights limitations in data and architecture. Future If true, human-level AI may require millions of times more compute, achievable by the 2030s. Gwern‚Äôs prediction: AGI possible by 2038 with current compute growth rates. Applications Scaling enables powerful language models for science, business, and government, but risks misuse. GPT-3 can automate tasks or generate propaganda. ANI, AGI, ASI to-be..\n","permalink":"https://kodinkod.github.io/posts/ai-general/","summary":"is all you need.","title":"Ai General"},{"content":"Base What are Diffusion Models? Lil\u0026rsquo;Log Distilled AI | Diffusion Models Score-based generative models Arena imgsys - a generative image model arena by rankings. (fal). Image Generation Video Generation Music Generation On device ","permalink":"https://kodinkod.github.io/posts/diffusion_lib/","summary":"Useful tools, papers, code about Diffusion models","title":"Diffusion Models"},{"content":"tg bots –ö–∞–∫ —Å–æ–∑–¥–∞—Ç—å —Ç–µ–ª–µ–≥—Ä–∞–º-–±–æ—Ç–∞ –Ω–∞ –æ–±–ª–∞—á–Ω–æ–º —Å–µ—Ä–≤–µ—Ä–µ ","permalink":"https://kodinkod.github.io/posts/2025-01-18-indi-hacking-tools/","summary":"Useful tools, papers, code about NLP.","title":"Indi hacking tools"},{"content":"Documents Docling - parses documents and exports them to the desired format with ease and speed. ","permalink":"https://kodinkod.github.io/posts/2025-01-18-nlp-tools/","summary":"Useful tools, papers, code about NLP.","title":"NLP tools"},{"content":"Agentic programs are the gateway to the outside world for LLMs, code, people. We can identify several levels of agency:\nAgency Level Description How that\u0026rsquo;s called Example Pattern ‚òÜ‚òÜ‚òÜ LLM output has no impact on program flow Simple processor process_llm_output(llm_response) ‚òÖ‚òÜ‚òÜ LLM output determines basic control flow Router if llm_decision(): path_a() else: path_b() ‚òÖ‚òÖ‚òÜ LLM output determines function execution Tool call run_function(llm_chosen_tool, llm_chosen_args) ‚òÖ‚òÖ‚òÖ LLM output controls iteration and program continuation Multi-step Agent while llm_should_continue(): execute_next_step() ‚òÖ‚òÖ‚òÖ One agentic workflow can start another agentic workflow Multi-Agent if llm_trigger(): execute_agent() Table from https://github.com/huggingface/smolagents\n‚õî When to use agents Advantages (+) of Using Agents Disadvantages (-) of Using Agents Flexibility: Agents can adapt to complex and uncertain tasks that don‚Äôt fit predefined workflows. Complexity: Agents are harder to develop and debug compared to deterministic systems. Autonomy: Agents can make independent decisions based on data and context. Unpredictability: Agents may make unexpected or incorrect decisions due to ambiguous data. Scalability: Agents can handle a wide range of tasks by leveraging various tools (APIs, databases, etc.). Cost: Using agents can be more expensive due to the need for powerful computational resources. Real-world applicability: Agents excel at tasks requiring analysis of multiple factors and context. Risk of errors: Agents may make mistakes, especially with incomplete or conflicting data. Integration with external systems: Agents can interact with APIs, databases, and other tools. Control challenges: It‚Äôs harder to control and explain agent behavior in complex scenarios. Suitable for complex tasks: Agents are useful when tasks require analyzing multiple variables. Overkill for simple tasks: Agents may add unnecessary complexity for straightforward tasks. About agents Building effective agents Open-source LLMs as LangChain Agents –ò–ò-–∞–≥–µ–Ω—Ç—ã: –æ—Ç —Ç–µ–æ—Ä–∏–∏ –∫ –ø—Ä–∞–∫—Ç–∏–∫–µ Library smolagents smolagents is a library that enables you to run powerful agents in a few lines of code.\n","permalink":"https://kodinkod.github.io/posts/2025-01-18-ai-agnets/","summary":"Useful tools, papers, code about LLM Agents.","title":"LLM Agents tools"},{"content":"My experience üí° RnD CV, Gen-AI, Diffusion models in T-bank. RnD CV in AgroTech startup. Education üìò SFEDU, Mathematics Mechanics and Computer Science - applied mathematics and computer science\nContribute in open source üî¶ Open Metric Learning criterion.weight in CheckPoint resnet50 #422 bboxes_to_segmentation_with_SAM ","permalink":"https://kodinkod.github.io/posts/denis-kodin/","summary":"\u003ch3 id=\"my-experience-\"\u003eMy experience üí°\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eRnD CV, Gen-AI, Diffusion models in T-bank.\u003c/li\u003e\n\u003cli\u003eRnD CV in AgroTech startup.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"education-\"\u003eEducation üìò\u003c/h3\u003e\n\u003cp\u003eSFEDU, Mathematics Mechanics and Computer Science - applied mathematics and computer science\u003c/p\u003e\n\u003ch3 id=\"contribute-in-open-source-\"\u003eContribute in open source üî¶\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eOpen Metric Learning \u003ca href=\"https://github.com/OML-Team/open-metric-learning/issues/422\"\u003ecriterion.weight in CheckPoint resnet50 #422\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/broutonlab/bboxes_to_segmentation_with_SAM\"\u003ebboxes_to_segmentation_with_SAM\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"Denis Kodin"}]