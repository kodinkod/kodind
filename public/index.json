[{"content":"Situational awareness Updated June 6, 2024–±, Situational awareness. \u0026hellip; Before long, the world will wake up. But right now, there are perhaps a few hundred people, most of them in San Francisco and the AI labs, that have situational awareness. Through whatever peculiar forces of fate, I have found myself amongst them. A few years ago, these people were derided as crazy‚Äîbut they trusted the trendlines, which allowed them to correctly predict the AI advances of the past few years. Whether these people are also right about the next few years remains to be seen. But these are very smart people‚Äîthe smartest people I have ever met‚Äîand they are the ones building this technology. Perhaps they will be an odd footnote in history, or perhaps they will go down in history like Szilard and Oppenheimer and Teller. If they are seeing the future even close to correctly, we are in for a wild ride \u0026hellip;\nJanuary 21, 2025, Announcing The Stargate Project - company which intends to invest $500 billion over the next four years building new AI infrastructure for OpenAI in the United States. The Scaling Hypothesis The The Scaling Hypothesis suggests that increasing the scale of neural networks (more parameters, data, and compute) improves their performance and ability to solve complex tasks. This idea is supported by models like GPT-3, which demonstrate meta-learning (few-shot learning) and generalization without complex architectures. The hypothesis implies that intelligence can emerge from simple algorithms applied at massive scales.\nAspect Description Example/Evidence Core Idea Scaling up (parameters, data, compute) improves performance, even with simple architectures. GPT-3: 175B parameters, trained on internet text, demonstrates few-shot learning. Meta-Learning Models like GPT-3 can learn new tasks from few examples, showing generalization capabilities. GPT-3 solves arithmetic and translation tasks without specialized training. Empirical Laws Performance follows power laws: improvements depend on scale of parameters, data, and compute. OpenAI‚Äôs scaling laws: loss decreases with more parameters and data. Criticism Some argue scaling alone won‚Äôt lead to true AI, as models lack long-term memory or understanding. Fran√ßois Chollet (DeepMind) highlights limitations in data and architecture. Future If true, human-level AI may require millions of times more compute, achievable by the 2030s. Gwern‚Äôs prediction: AGI possible by 2038 with current compute growth rates. Applications Scaling enables powerful language models for science, business, and government, but risks misuse. GPT-3 can automate tasks or generate propaganda. ANI, AGI, ASI to-be..\n","permalink":"https://kodinkod.github.io/posts/ai-general/","summary":"is all you need.","title":"Ai General"},{"content":"Base What are Diffusion Models? Lil\u0026rsquo;Log Distilled AI | Diffusion Models Score-based generative models Arena imgsys - a generative image model arena by rankings. (fal). Image Generation Video Generation Music Generation On device ","permalink":"https://kodinkod.github.io/posts/diffusion_lib/","summary":"Useful tools, papers, code about Diffusion models","title":"Diffusion Models"},{"content":"tg bots –ö–∞–∫ —Å–æ–∑–¥–∞—Ç—å —Ç–µ–ª–µ–≥—Ä–∞–º-–±–æ—Ç–∞ –Ω–∞ –æ–±–ª–∞—á–Ω–æ–º —Å–µ—Ä–≤–µ—Ä–µ ","permalink":"https://kodinkod.github.io/posts/2025-01-18-indi-hacking-tools/","summary":"Useful tools, papers, code about NLP.","title":"Indi hacking tools"},{"content":"Documents Docling - parses documents and exports them to the desired format with ease and speed. ","permalink":"https://kodinkod.github.io/posts/2025-01-18-nlp-tools/","summary":"Useful tools, papers, code about NLP.","title":"NLP tools"},{"content":"About agents Building effective agents Open-source LLMs as LangChain Agents Chip Huyen, Agents - a great tutorial about agents. Jan 7, 2025. Agents Computer-Using Agent - introduced a research preview of Operator‚Å†(opens in a new window), an agent that can go to the web to perform tasks for you.OpenAI, January 23, 2025. Library smolagents is a library that enables you to run powerful agents in a few lines of code. ","permalink":"https://kodinkod.github.io/posts/2025-01-18-ai-agnets/","summary":"Useful tools, papers, code about LLM Agents.","title":"LLM Agents tools"},{"content":"My experience üí° RnD CV, Gen-AI, Diffusion models in T-bank. RnD CV in AgroTech startup. Education üìò SFEDU, Mathematics Mechanics and Computer Science - applied mathematics and computer science\nContribute in open source üî¶ Open Metric Learning criterion.weight in CheckPoint resnet50 #422 bboxes_to_segmentation_with_SAM ","permalink":"https://kodinkod.github.io/posts/denis-kodin/","summary":"\u003ch3 id=\"my-experience-\"\u003eMy experience üí°\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eRnD CV, Gen-AI, Diffusion models in T-bank.\u003c/li\u003e\n\u003cli\u003eRnD CV in AgroTech startup.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"education-\"\u003eEducation üìò\u003c/h3\u003e\n\u003cp\u003eSFEDU, Mathematics Mechanics and Computer Science - applied mathematics and computer science\u003c/p\u003e\n\u003ch3 id=\"contribute-in-open-source-\"\u003eContribute in open source üî¶\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eOpen Metric Learning \u003ca href=\"https://github.com/OML-Team/open-metric-learning/issues/422\"\u003ecriterion.weight in CheckPoint resnet50 #422\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/broutonlab/bboxes_to_segmentation_with_SAM\"\u003ebboxes_to_segmentation_with_SAM\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"whoami"}]